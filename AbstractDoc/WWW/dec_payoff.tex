%!TEX root = main.tex

\section{Decreasing Payoff}
\label{sec-dec}

%While interesting, readers could argue that the payoff function considered before is not completely modelling bitcoin, because in bitcoin 
%the reward given for mining blocks is reduced by half every 200.000 blocks or so. Thus the natural question, do the results above continue to hold under these 
%circumstances? Our answer is a resounding no, and now forking can be a valid strategy depending on the hash power of a player (and the other parameters of the game). 

The miner fees in most cryptocurrencies, including Bitcoin, is not constant, but diminishes over time. 
It is thus natural to ask whether mining on top of the existing blockchain continues to be an optimal strategy under these 
circumstances? Our answer is a resounding no, and as we show in this section, when the miner fees decrease over time, forking can be a valid strategy depending on the hash power of the player (and other parameters of the game). 

For simplicity, we model the diminishing miner fees as a constant factor that is lowered after every new block in the blockchain. That is, in this section 
we use the following reward function $\rpa$ for all players $p \in \bP$, denoted as the \textbf{$\alpha$-discounted reward}: 
%\begin{eqnarray*}
%\rpa(q) & = & 
%{\displaystyle c \cdot \sum_{i=1}^{|\meet(q)|} \alpha^i \cdot \chi_p(\meet(q),i)} \end{eqnarray*}
\begin{eqnarray*}
\rpa(q) & = & 
c \cdot \sum_{b \in \meet(q)} \alpha^{|b|} \cdot \chi_p(b),
\end{eqnarray*}
where $c$ is a positive real number and $\alpha \in (0,1]$. Notice that in the case of Bitcoin, $\alpha$ has to make the reward halve every 210.000 blocks, and is therefore very close to 1. This is usually the case in other cryptocurrencies as well. 

%\medskip
%
%(say that the approach we use is the same as above: we consider a two player game and group a bunch of well behaved players 
%into a single player. Also state that the subtree lemma holds also for this payoff)

\subsection{When is forking a good strategy?}
\label{sec-forkingstrategies}

To calculate when forking is a viable option, we consider a scenario when one of our $m$ players considers to deviate from the default strategy, while the remaining players all follow the default strategy. 
%This is similar the question a miner with a lot of hash power is likely to ask, since she would like to determine whether it suites her to try and force a fork in order to own more blocks in the new blockchain. 
The first observation is that in this case we can reduce the $m$ player game to a two player game, where all the players following the default strategy are represented by a single player behaving as the protocol dictates, with the combine hash power of all these players. Therefore  in this section we will consider that the mining game is played by two players 0 and 1, where 0 represents the miners behaving according to the default strategy, and 1 the miner trying to determine whether doing a fork is economically more viable than mining on the existing blockchain. We always assume that the player 1 has hash power $h$, while player 0 has hash power $1-h$.

In order to determine whether there is an incentive for player 1 to do a fork, we first need to determine her utility when she is playing according to the default strategy $\cdf = (\df_0,\df_1)$. 

\begin{lemma}\label{lem:default_utility}
If $h$ is the hash power of player 1, and the strategy deployed by the two players is $\cdf$, then the utility of player 1 is given by the expression:
$$u_1(\cdf) = h\cdot c\cdot\frac{\alpha\cdot\beta}{(1-\alpha\cdot\beta)}.$$
\end{lemma}
As in the case of constant reward, this corresponds to $h$ times the utility of winning all the blocks in the single blockchain generated by this strategy.

%In this section, players $\{0,1\}$ play according to the default strategy, defined in section \ref{sec-defstrategy}. For any state $q$ of the game, $\mathcal{T}(q)$ consists of a single branch and therefore \bchain$(q)$ is always defined. Moreover, given the behavior of players we can write $q$ uniquely as a binary sequence $w\in\{0,1\}^{\mid q\mid }$ encoding the chronological history of the game until $q$ is reached, where $p\in\{0,1\}$ stands for ``player $p$ appends a block''. In other words, there are bijections $ \bQ \simeq\bchain(\bQ)\simeq \{0,1\}^\ast$. This encoding proves useful as we have
%\begin{eqnarray*}
%	r_p(w) &=&	c\cdot \sum_{j=1}^{\mid w\mid}w[j] \alpha^j  \\
%	\pr^{\df}(w \mid \varepsilon) &=&	h^{H(w)}(1-h)^{|w|-H(w)}
%\end{eqnarray*}
%where $H(x)$ denotes the Hamming weight of integer $x$, defined as the amount of non-zero bits of $x$. We prove the following.

Now suppose player $1$ deviates from the default strategy, and considers a strategy based on forking the blockchain once player $0$ mines a block. 
How would this new strategy would look for player~$1$? To consider a realistic scenario, in this section we study the following strategies. 
%
%Juan: this is better as a comment once we analyse stuff
%For simplicity we only consider the situation where both players $0$ and $1$ begin the game in the genesis block, but this is without generality: 
%any state before player $1$ deviates is just a sequence of blocks, a blockchain, and thus we can always sum up any reward player $0$ or $1$ is collecting thanks to the blocks in this 
%sequences.  
%
%\begin{itemize}
%\item 
First we consider a strategy $\af$ (for \emph{always fork}), where player~$1$ forks as soon as player $0$ mines a block in the blockchain, and she continues mining on the new branch until it becomes the blockchain (if possible). Here player $1$ is willing to fork every time player $0$ produces a block in the blockchain. In other words, 
in $\af$ player~$1$ tries to have all blocks in the blockchain. This strategy is depicted in Figure \ref{fig:always_fork}.
%\item 
Second, we consider a family of strategies $\pf{1}$, $\pf{2}$, etc. In the strategy $\pf{i}$, player $1$ behaves just as in $\af$, but will do a fork at most $i$ times, %a specific number of times, 
switching to $\df$ once 
all these forks have been completed. Coming back to Figure \ref{fig:always_fork}, if player $1$ plays according to $\pf{1}$ instead of $\af$, then she would fork upon block $0$, but 
would continue mining on block $1110$ instead of forking again. 
%\end{itemize}

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 0.6,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (-0.2,0) (R) {$\varepsilon$};
	\node [state] at (1.5,0.75) (1) {$1$};
	\node [state] at (1.5,-0.75) (0) {$0$};

	\node [state] at (3.3,0.75) (11) {$11$};		
	\node [state] at (5.2,0.75) (111) {$111$};
	\node [state] at (7.4,0.75) (1110) {$1110$};

	\node [state] at (7.4,2) (1111) {$1111$};
	\node [state] at (9.8,2) (11111) {$11111$};
	
	%\node [state] at (4.6,1.5) (111) {$111$};
	%\node [state] at (6.2,1.5) (1111) {$1111$};

	% Graph edges
	\path[->]
	(R) edge (0)
	(1) edge (11)
	(11) edge (111)	
	(111) edge (1110)
	(1111) edge (11111);
	
	\path[->,dashed]
	(R) edge (1)
	(111) edge (1111);
	
%	(11) edge (111)
%	(111) edge (1111);

\end{tikzpicture} 
\end{center}
\caption{Dashed arrows are used to indicate when player~1 do a fork. Initially, player $0$ mines on the genesis block $\varepsilon$ including block $0$. Player 1 then decides to fork on the genesis block, and continue mining in this new brach until it becomes the blockchain, which happens in this case when player 1 mines on top of  block $1$ generating block $11$. But then again player 1 loses a block when player 0 mines on top of block $111$ generating block $1110$, so she decides to fork on top of block $111$ until the new brach becomes the blockchain. Up to this point in the mining game, the blockchain is given by the path $\{\varepsilon, 1, 11, 111, 1111, 11111\}$ that only contains blocks belonging to player 1.
%
%until the new brach becomes the blockchain
%which is represented by a dashed arrow, after she manages to Player Following the block 0 player 1 tries to win a fork starting at $\varepsilon$. After winning the initial fork, if she loses another block (as in the block 110), she will try to fork again. When playing $\baf$, the word $w$ describing this state is $011011$.}
\label{fig:always_fork}}
\end{figure}

%\medskip
%\noindent
%\textbf{Strategy $\af$}. 
\subsubsection{On the utility of always forking}
%As we explained, the idea of $\af$ for player $1$ is to fork every time she loses a block, trying to own all blocks in the blockchain. 
The question we want to answer is twofold. On the one hand, we want to know whether $\af$ is a better strategy than $\df_1$ for player $1$, under the assumption that player $0$ uses $\df_0$, and under some specific values of $\alpha$, $\beta$ and $h$. Since we know how to compute the utility for $\bdf = (\df_0,\df_1)$, this amounts to computing 
%all we need is a way to compute 
the utility for player $1$ for the combined strategy $(\df_0,\af)$. On the other hand, and 
%But 
perhaps more interestingly, we can also answer a more analytical question: given realistic values of $\alpha$ and $\beta$, how much hash power does player $1$ need to consider $\af$ instead of $\df_1$? 
Answering both questions requires us to learn how to compute the utility for strategy $(\df_0,\af)$, so this is what we do next. 

\begin{theorem}\label{thm:always_fork}
If $h$ is the hash power of player 1, then:
%, $\baf=(\df_0,\af)$ be the strategy deployed in the mining game, and let $Cat$ be the generating function of Catalan numbers. Then:
\begin{eqnarray*}
u_1(\baf) &=& \frac{\varphi}{1-\Gamma}, 
\end{eqnarray*}
where $\cat$ is the generating function of Catalan numbers\footnote{The generating function of Catalan numbers \cite{Catalans} is given by $Cat(x) = \frac{1-\sqrt{1-4x}}{2x}$.}, and
\begin{align*}
\varphi & = \ \frac{\alpha \cdot \beta \cdot h }{(1-\alpha)\cdot (1-\beta)} \cdot \bigg[\cat(\beta^2 \cdot h \cdot (1-h)) \ -\\
& \hspace{120pt} \alpha\cdot \cat(\alpha\cdot \beta^2 \cdot h \cdot (1-h))\bigg]\\
\Gamma & = \ \alpha \cdot \beta \cdot h \cdot \cat(\alpha\cdot \beta^2 \cdot h \cdot (1-h)).
\end{align*}
\end{theorem}

%ADD FORK ONCE

%Of course, if player 1 knows that her utility will be higher when she forks in the genesis block as opposed to playing the default strategy, then why not repeat the fork every time she loses a block? After all, if she is likely to win one fork, she is likely to win multiple forks as well. Let $F_\infty$ be a strategy for player 1 where she tries to have all the blocks in the blockchain. That is, in $\af$, player one will fork every time she loses a single block. This strategy is depicted in Figure \ref{fig:always_fork}. We can now obtain the following.


\begin{proof}
Let $Q_\baf = \{q \in \bQ \mid \pr^\baf(q) > 0\}$ be the set of all states that can be reached from the genesis block using the strategy $\baf$, and from the proof of Theorem \ref{thm-conts_dom_str} recall the definition of sequence $\rho$ for a state $q$, and recall the construction of string $b_\rho$ from such a sequence $\rho$.
%the mapping $\sigma: Q_\baf \rightarrow 2^{\bQ_\bdf}$ introduced  in the proof of Theorem \ref{thm-conts_dom_str}, now in the context of strategy $\baf$. From the function $\sigma$, we define $\tau:Q_\baf \mapsto 2^{\{0,1\}^*}$ as follows:
By using these elements, we define $\tau:Q_\baf \mapsto 2^{\{0,1\}^*}$ as follows:
\begin{eqnarray*}
\tau(q) & = & \{ b_\rho \mid \rho \text{ is a sequence for } q\}.
\end{eqnarray*}
Intuitively, $\tau(q)$ is the set of all moves that players 0 and 1 can do in $|q|$ steps according to $\baf$ that lead them to the state $q$ when starting in the genesis block. As such, they are coded as sequences of zeros and ones that tell us which player puts a block at the stage $i$ of the game, for $i \in \{ 1,\ldots, |q|\}$. It is straightforward to verify the following:
\begin{myclaim}\label{claim-words-app} For every $q, q'\in Q_\baf$, it holds that:
\begin{itemize}
\item[(a)] If $q\neq q'$, then $\tau(q)$ is disjoint from $\tau(q')$.
\item[(b)] $\pr^{\baf}(q) = \sum_{w \in \tau(q)} \pr(w)$, where $\pr(w)$ for a word $w$ with $n_0$ zeroes and $n_1$ ones is  defined as 
$h^{n_1}(1-h)^{n_0}$.
\end{itemize}
\end{myclaim}
In particular, Claim \ref{claim-words-app} (a) can be proved exactly in the same way Claim \ref{claim-nonempty-inter-gen} is proved. Notice that Claim \ref{claim-words-app} (a)
%The first property in Claim \ref{claim-words} 
tells us that a sequence of actions of players 0 and 1 uniquely determines a state of the game. 
Moreover,  Claim \ref{claim-words-app} (b)
%The second property 
tells us that the probability of a state $q$ is the sum of probabilities of all the sequences of actions of players 0 and 1 that end up in $q$ when started in the genesis block. Observe that since the actions of players 0 and 1 are independent trials, with  probabilities $1-h$ and $h$, respectively, the probability of a state where player 0 wins $n_0$ rounds and player 1 wins $n_1$ rounds is $h^{n_1}(1-h)^{n_0}$, as stated in the claim.

From Claim \ref{claim-words-app}, we conclude that the utility of player $1$ can be rewritten as follows:
\begin{eqnarray*}
u_1(\baf) & = & \sum_{q \in \bQ} \beta^{|q|-1} \cdot  r_1(q) \cdot \pr^{\cdf}(q)\\
& = & \sum_{q \in \bQ} \beta^{|q|-1} \cdot  r_1(q) \cdot \bigg(\sum_{w \in \tau(q)} \pr(w)\bigg)\\
& = &  \sum_{q \in \bQ} \sum_{w \in \tau(q)} \beta^{|q|-1} \cdot  r_1(q) \cdot \pr(w)\\
& = &  \sum_{q \in \bQ} \sum_{w \in \tau(q)} \beta^{|w|} \cdot  r_1(w) \cdot \pr(w)\\
& = & \sum_{w \in \{0,1\}^*} \beta^{|w|} \cdot  r_1(w) \cdot \pr(w),
\end{eqnarray*}
given that $|w| = |q| -1$ for every $w \in \tau(q)$, and assuming that $r_1(w)$ is defined as $r_1(q)$ for the only state $q$ such that $w \in \tau(q)$.


%Since $\varepsilon\subseteq q$, for any state $q\in \bQ$, by the definition of utility we have that: 

%$$u_1(\baf\mid\varepsilon) = \sum_{q\in \bQ}\beta^{|q|}\cdot r(q)\cdot \pr^{\baf}(q\mid \varepsilon).$$

%Applying the idea of coding the states in a two player game as sequences of binary numbers, we can write the above as:

%\begin{equation}\label{eq:def_utility}
%u_1(\baf\mid\varepsilon) = \sum_{w\in \{0,1\}^*}\beta^{|w|}\cdot r(w)\cdot \pr^{\baf}(w\mid \varepsilon).
%\end{equation}

We will now try to describe all the states in which player 1 receives a non-zero reward in terms of words. For this, let us consider the set $S$ of all words $w \in \{0,1\}^*$ that represent states $q$ (via $\tau$) in which player $1$ owns at least one block in the blockchain for the {\em first time}. 
The smallest of them is $w = 1$, which represents the state in Figure \ref{fig:proof-theorem-4} (a). This state is created when player $q$ wins the first move of the game, successfully mining upon the genesis block. Next is the word $011$, representing the state in Figure \ref{fig:proof-theorem-4} (b). To arrive at this state player $0$ must have mined the first block, player $1$ forked, and then player $1$ 
won the following block (on her forking branch). The next words in $S$ are $00111$ and $01011$, both representing the state in Figure \ref{fig:proof-theorem-4} (c). 
In general, the words in the set $S$ have the form $d\cdot 1$, where $d$ is a \emph{Dyck word} \cite{Dyck}: a word with the same number of $0$s and $1$s, but such that 
no prefix of $d$ has more $1$s than $0$s (this intuitively means that at any point player $1$ did not have more blocks than $0$). 
Note that the only Dyck word of length $0$ is $\varepsilon$, the next Dyck word by length is $01$, and then $0011$ and $0101$, etc. We use $\Dyck$ to denote the set of all Dyck words. Notice that by definition all elements of $\Dyck$ are of even length.

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 0.61,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (-4.5,0) (aR) {$\varepsilon$};
	\node [state] at (-3,0) (a1) {$1$};
	\node [state] at (-3.8,-1.5) {(a)};

	% Graph edges
	\path[->]
	(aR) edge (a1);  	

    % The graph
	\node [state] at (0,0) (bR) {$\varepsilon$};
	\node [state] at (1.5,0.75) (b1) {$1$};
	\node [state] at (1.5,-0.75) (b0) {$0$};

	\node [state] at (3,0.75) (b11) {$11$};	
	\node [state] at (1.6,-1.5) {(b)};
	
	% Graph edges
	\path[->]
	(bR) edge (b0)
	(bR) edge (b1)
	(b1) edge (b11);

    % The graph
	\node [state] at (-3.15,-3.5) (cR) {$\varepsilon$};
	\node [state] at (-1.65,-2.75) (c1) {$1$};
	\node [state] at (-1.65,-4.25) (c0) {$0$};

	\node [state] at (-0.15,-4.25) (c00) {$00$};
	
	\node [state] at (-0.15,-2.75) (c11) {$11$};	
	\node [state] at (1.35,-2.75) (c111) {$111$};	
	\node [state] at (-0.75,-5) {(c)};
	
	% Graph edges
	\path[->]
	(cR) edge (c0)
	(c0) edge (c00)
	(cR) edge (c1)
	(c1) edge (c11)
	(c11) edge (c111);

\end{tikzpicture} 
\end{center}

\caption{States in a game played according to strategy $\baf$. \label{fig:proof-theorem-4}}
\end{figure}

Since all states where player $1$ receives a reward involve putting a block in the blockchain, all words 
$w$ with $r_1(w) > 0$ are therefore of the form $d\cdot 1\cdot w'$ with $d \in \Dyck$. Now let $q$ be the only state such that  $d\cdot 1\cdot w' \in \tau(q)$.
% be the state represented by $d\cdot 1\cdot w'$. 
State $q$ can be seen as a tree with two branches: one only with blocks earned by player $0$, and the other one 
with at least ${\frac{|d|}{2}+1}$ blocks owned by player $1$ (plus maybe more, depending on $w'$). 
We can then calculate the reward for $q$ as: 
\begin{eqnarray*}
r_1(q) & = & \bigg(\sum_{i=1}^{\frac{|d|}{2}+1}\alpha^i \bigg)+ \alpha^{\frac{|d|}{2}+1}\cdot r_1(w').
\end{eqnarray*}
Hence, we obtain:
\begin{eqnarray*}
u_1(\baf) = \sum_{d\in \Dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot \big[r(d\cdot 1) + \alpha^{\frac{|d|}{2}+1}\cdot r(w)\big] \cdot \\ \pr(d\cdot 1)\cdot \pr(w).
\end{eqnarray*}
%
The product of probabilities is obtained since winning a block is an independent trial. Splitting up the sums we get:
\begin{multline*}
 u_1(\baf) = 
 \sum_{d\in \Dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot r(d\cdot 1) \cdot \pr(d\cdot 1)\cdot \pr(w)
 \mbox{ } +\\
  \sum_{d\in \Dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot r(w) \cdot \pr(d\cdot 1)\cdot \pr(w).
\end{multline*}
%
We denote the first term in the equation above by $\varphi$. 
Next, in the expression for $u_1(\baf)$ we will split the second term into the elements that depend only on $d$, and the ones that depend only on $w$, getting:
%
\begin{align*}
 & u_1(\baf) = \varphi \mbox{ } +\\
 & \sum_{d\in \Dyck} \beta^{|d|+1}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot \pr(d\cdot 1) \cdot 
 \sum_{w\in \{0,1\}^*} \beta^{|w|} \cdot r(w)  \cdot \pr(w).
\end{align*}
%
Since the rightmost sum is precisely $u_1(\baf)$, we have:
%
\begin{align*}
 u_1(\baf) = \varphi \mbox{ } + 
 \sum_{d\in \Dyck} \beta^{|d|+1}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot \pr(d\cdot 1) \cdot  u_1(\baf).
\end{align*}
%
By denoting with $\Gamma$ the rightmost term of the sum above we get the equation:
$$u_1(\baf\mid\varepsilon) = \frac{\varphi}{1-\Gamma}.$$
Let us now find a closed form for $\Gamma$.
%
\begin{align*}
\Gamma = & \sum_{d\in \Dyck} \beta^{|d|+1}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot \pr(d\cdot 1)\\
 = & \mbox{ } \alpha\cdot \beta \sum_{d\in \Dyck} \beta^{|d|}\cdot  \alpha^{\frac{|d|}{2}}\cdot \pr(d\cdot 1) \\
  = & \mbox{ } \alpha\cdot \beta \sum_{\ell = 0}^{\infty} \sum_{d\in \Dyck_{2\ell}} (\alpha\cdot \beta^2)^{\ell}\cdot h^{\ell}\cdot (1-h)^{\ell}\cdot h\\
   = & \mbox{ } \alpha\cdot \beta \sum_{\ell = 0}^{\infty} |\Dyck_{2\ell}| (\alpha\cdot \beta^2)^{\ell}\cdot h^{\ell}\cdot (1-h)^{\ell}\cdot h\\
    = & \mbox{ } \alpha\cdot \beta \cdot h \cdot Cat(\alpha\cdot\beta^2 \cdot h \cdot (1-h)).\\
\end{align*}
%
Here the third equality follows since all Dyck words are of even length (we use $\Dyck_{2\ell}$ to denote the set of all Dyck words of length $2\ell$). The final equality is obtained using the fact that the $\ell$th Catalan number is equal to the number of Dyck words of length $2\ell$ \cite{??}, thus the summation in the previous line defines the generating function of Catalan numbers.
%
The closed form for $\varphi$ is computed in a similar way, constructing again generating functions of Catalan numbers for  specific parameters. 
\end{proof}



%%%OFF :) THE VERSION JUAN HAD BEFORE MY PASS
\begin{comment}
\begin{proof}
Let $Q_\baf = \{q \in \bQ \mid \pr^\baf(q) > 0\}$ be the set of all states that can be reached from the genesis using strategy $\baf$, and recall 
the mapping $\sigma: Q_\baf \rightarrow \{0,1\}^*$ introduced  in the proof of Theorem \ref{thm-conts_dom_str}, now in the context of strategy $\baf$. From the definition of $\sigma$ we have that for any state $q \in Q_\baf$ one verifies 
$\pr^{\baf}(q \mid \varepsilon) = \sum_{w \in \sigma(q)} \pr(w \mid \varepsilon)$, where $\pr(w \mid \varepsilon)$ for a word $w$ with $n_0$ zeroes and $n_1$ ones is simply 
$h^{n_1}(1-h)^{n_0}$. Further, by Claim \ref{claim-nonempty-inter-gen} the inverse  $\sigma^{-1}: \{0,1\}^* \rightarrow Q_\baf$ is a total function.  
All of this means that we can rewrite the utility of player $1$ using $\baf$ as: 
\begin{multline*}
u_1(\baf \mid \varepsilon) = \sum_{q \in \bQ} \beta^{|q|-1} \cdot  r_1(q) \cdot \pr^{\cdf}(q \mid \varepsilon) = \\ 
\sum_{q \in \bQ} \sum_{w \in \sigma(q)} \beta^{|w|} \cdot  r_1(w) \cdot \pr(w \mid \varepsilon) =  \\
\sum_{w \in \{0,1\}^*} \beta^{|w|} \cdot  r_1(w) \cdot \pr^{\baf}(w \mid \varepsilon),
\end{multline*}
where $r_1(w)$ is just a convenient shorthand for $r_1(\sigma^{-1}(w))$.


%Since $\varepsilon\subseteq q$, for any state $q\in \bQ$, by the definition of utility we have that: 

%$$u_1(\baf\mid\varepsilon) = \sum_{q\in \bQ}\beta^{|q|}\cdot r(q)\cdot \pr^{\baf}(q\mid \varepsilon).$$

%Applying the idea of coding the states in a two player game as sequences of binary numbers, we can write the above as:

%\begin{equation}\label{eq:def_utility}
%u_1(\baf\mid\varepsilon) = \sum_{w\in \{0,1\}^*}\beta^{|w|}\cdot r(w)\cdot \pr^{\baf}(w\mid \varepsilon).
%\end{equation}

Now, let us consider the set $S$ of all words $w \in \{0,1\}^*$ that represent states $q$ (via $\sigma$) in which player $1$ owns at least one block in the blockchain for the first time. 
The easiest of them is $w = 1$, which represents the state in Figure XXX. This state is created when player $q$ wins the first move of the game, sucesfully mining upon the genesis block. Next is word $011$, representing the state in Figure YY. To arrive at this state player $0$ must have mined the first block, player $1$ forked and then player $1$ 
forked again. The next words in $S$ are $00111$ and $01011$, both representing the state of Figure ZZZ. 
In general, the words in the set $S$ have the form $d1$, where $d$ is a \emph{Dyck word}: a word with the same number of $0$s and $1$s, but such that 
no prefix of $d$ has more $1$s than $0$s (this intuitively signifies that player $1$ never had more blocks than $0$ at any point). 
Note that the only dyck word of length $0$ is $\epsilon$, the next dyck word by length is $01$, and then $0011$ and $0101$. 

\begin{figure}
\begin{subfigure}{0.2\columnwidth}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 1.0,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (-2,0) (R) {$\varepsilon$};
	\node [state] at (-0.5,0) (1) {$1$};

	% Graph edges
	\path[->]
	(R) edge (1);  	
\end{tikzpicture} 
\end{subfigure} 
\qquad
\begin{subfigure}{0.3\columnwidth}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 1.0,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (0,0) (R) {$\varepsilon$};
	\node [state] at (1.5,0.75) (1) {$1$};
	\node [state] at (1.5,-0.75) (0) {$0$};

	\node [state] at (3,0.75) (11) {$11$};	
	
	% Graph edges
	\path[->]
	(R) edge (0);  	
	
	\path[->]
	(R) edge (1)
	(1) edge (11);

\end{tikzpicture} 
\end{subfigure}

\begin{subfigure}[b]{0.3\textwidth}
\begin{center}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 1.0,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (0,0) (R) {$\varepsilon$};
	\node [state] at (1.5,0.75) (1) {$1$};
	\node [state] at (1.5,-0.75) (0) {$0$};

	\node [state] at (3,-0.75) (00) {$00$};
	
	\node [state] at (3,0.75) (11) {$11$};	
	\node [state] at (4.5,0.75) (111) {$111$};	
	
	% Graph edges
	\path[->]
	(R) edge (0)
	(0) edge (00);  	
	
	\path[->]
	(R) edge (1)
	(1) edge (11)
	(11) edge (111);

\end{tikzpicture} 
\end{center}
\end{subfigure}

\caption{XXX}
\label{fig:proof-theorem-4}
\end{figure}

Since all states where player $1$ receives a reward involve putting a block in the blockchain, all words 
$w$ with $r_1(w) > 0$ are therefore of the form $d 1 w'$. Now let $q = \sigma^{-1}(d1w')$ be the state represented by $d1w'$. 
State $q$ can be seen as a tree with two branches: one only with blocks earned by player $0$, and the other one 
with at least ${\frac{|d|}{2}+1}$ blocks owned by player $1$ (plus maybe more, depending on $w$). 
We can then calculate the reward for $q$: 
$$r_1(q) = \big(\sum_{i=1}^{\frac{|d|}{2}+1}\alpha^i \big)+ \alpha^{\frac{|d|}{2}+1}\cdot r_1(\sigma^{-1}(w)).$$
Therefore (and writing $r_1(w)$ for $r_(\sigma^{-1}(w)))$, we can write:
%
%Now notice that when playing $\baf$, player 1 will fork each time she loses a block. This in particular means, that the states where she will receive an award for her blocks will always start with her winning one or more forks. For instance, in Figure \ref{fig:always_fork}, player 1 receives an award in block 11, which corresponds to a state coded by the word $w=011$. A similar story holds true for block 111, which corresponds to a state coded by the word $w=01101$. In general, states where player 1 receives an award can be coded by a Dyck word $d$, where both 1 and 0 have the same number of blocks in the game, followed by a block won by 1. After that any word $w$ can follow, leaving us in a state coded by $d1w$. Notice that the reward of player 1 in this state can be split into the reward for $d1$ (since this will always be part of $\meet(q)$, for $q$ the state corresponding to $d1w$), and the reward for $w$, with a shift of $\alpha^{\frac{|d|}{2}+1}$, since the latter is the length of the longest common chain before $w$ starts. Therefore, we can write the above utility as follows:
%
\begin{eqnarray*}
u_1(\baf\mid\varepsilon) = \sum_{d\in \dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot \big[r(d1) + \alpha^{\frac{|d|}{2}+1}\cdot r(w)\big] \cdot \\ \pr^{\baf}(d1\mid \varepsilon)\cdot \pr^{\baf}(w\mid \varepsilon).
\end{eqnarray*}
%
The product of probabilities is obtained since winning a block is an independent trial. Splitting up the sums we get:
\begin{align*}
 & u_1(\baf\mid\varepsilon) = \\
 & \sum_{d\in \dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot r(d1) \cdot \pr^{\baf}(d1\mid \varepsilon)\cdot \pr^{\baf}(w\mid \varepsilon)
 \mbox{ } +\\
 & \sum_{d\in \dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot r(w) \cdot \pr^{\baf}(d1\mid \varepsilon)\cdot \pr^{\baf}(w\mid \varepsilon).
\end{align*}
%
We will denote the first term in the equation above by $\varphi$. 
%
%More precisely,
%$$\varphi = \sum_{d\in \dyck}  \sum_{w\in \{0,1\}^*}\beta^{|d|+1+|w|}\cdot r(d1) \cdot \pr^{\baf}(d1\mid \varepsilon)\cdot \pr^{\baf}(w\mid \varepsilon).$$

Next, in the expression for $u_1(\baf\mid\varepsilon)$ we will split the second term into the elements that depend only on $d$, and the ones that depend only on $w$, getting:
%
\begin{align*}
 & u_1(\baf\mid\varepsilon) = \varphi \mbox{ } +\\
 & \sum_{d\in \dyck} \beta^{|d|+1}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot \pr^{\baf}(d1\mid \varepsilon) \cdot 
 \sum_{w\in \{0,1\}^*} \beta^{|w|} \cdot r(w)  \cdot \pr^{\baf}(w\mid \varepsilon).
\end{align*}
%
But the rightmost sum is precisely $u_1(\baf\mid\varepsilon)$. Thus, 
%
\begin{align*}
 u_1(\baf\mid\varepsilon) = \varphi \mbox{ } + 
 \sum_{d\in \dyck} \beta^{|d|+1}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot \pr^{\baf}(d1\mid \varepsilon) \cdot  u_1(\baf\mid\varepsilon).
\end{align*}
%
We denoting by $\Gamma$ the rightmost term of the sum above we get the equation:
$$u_1(\baf\mid\varepsilon) = \frac{\varphi}{1-\Gamma}.$$
Let us now find a closed form for $\Gamma$.
%
\begin{align*}
\Gamma = & \sum_{d\in \dyck} \beta^{|d|+1}\cdot  \alpha^{\frac{|d|}{2}+1}\cdot \pr^{\baf}(d1\mid \varepsilon)\\
 = & \mbox{ } \alpha\cdot \beta \sum_{d\in \dyck} \beta^{|d|}\cdot  \alpha^{\frac{|d|}{2}}\cdot \pr^{\baf}(d1\mid \varepsilon) \\
  = & \mbox{ } \alpha\cdot \beta \sum_{\ell = 0}^{\infty} \sum_{d\in \dyck_{2\ell}} (\alpha\cdot \beta^2)^{\ell}\cdot h^{\ell}\cdot (1-h)^{\ell}\cdot h\\
   = & \mbox{ } \alpha\cdot \beta \sum_{\ell = 0}^{\infty} |\dyck_{2\ell}| (\alpha\cdot \beta^2)^{\ell}\cdot h^{\ell}\cdot (1-h)^{\ell}\cdot h\\
    = & \mbox{ } \alpha\cdot \beta \cdot h \cdot Cat(\alpha\cdot\beta^2 \cdot h \cdot (1-h)).\\
\end{align*}
%
Here the third equality follows since all Dyck words are of even length. The final equality is obtained using the fact that the $\ell$th Catalan number is equal to the number of Dyck words of length $2\ell$, thus the summation in the previous line defines the generating function of Catalan numbers.

The closed form for $\varphi$ is computed in a similar way, constructing again generating functions of catalan numbers of specific parameters. 
\end{proof}
\end{comment}

\medskip
\noindent
\textbf{When to fork}. To answer this question, we used the closed forms of $\bdf$ and $\baf$ to compute the utility for a fixed $\alpha$, $\beta$ and varying hash power. 
To make it realistic as possible, we computed realistic values for $\alpha$ and $\beta$. For $\alpha$ we calculate the compound version of the discount of the bitcoin protocol, that is, 
the value XXXX verifying that $\alpha^{210.000} = 1/2$. 
For $\beta$ we calculate the 10-minute equivalent rate that would give us the US real interest rate in 2017 when compound over a year, which amounts to YYYYY. 

Figure \ref{Fig-af-df-fo} shows the value of the utility for player one when both players use the default strategy (color) and when players behave according to $\baf$ (color). 
The point where the two lines meet is $h = ZZZZ$, which shows that our model captures and confirms the folklore fact that it is profitable to fork with more than half the hash power. However, it is interesting to note that this is not exactly the case: one should be willing to fork even with a little less than half the hash power. 


\medskip
\noindent
\textbf{Forking for a fixed number of times}. The other question we answer in this section is regarding the number of times one should be willing to fork. 
This means analysing the family $F^1, F^2, \dots$ of strategies for player one, where $F^i$ dictates to follow $\af$ until the $i$-th fork is completed (and won), and then 
switching to $\df_1$. 

Using a similar strategy as we did for $\af$, we can calculate the utility for player one under each of the strategies $(\df_0,F^i)$. However, if one now looks at the (color) line 
of Figure \ref{Fig-af-df-fo}, one sees there is no need to do this as all of these are subsumed by $\af$: if $\af$ is actually better than default, then the more we fork the more utility we gain, and on the other hand, 
is $\af$ is worse than default, then  performing less forks is always the better alternative. 

It turns out we can capture this behaviour in a slightly more general way, as long as the strategies involve some repetitive behaviour. 
\juan{Etiennes Result!!!} 


\subsection{Giving up for more utility}. 

We have seen that one should start forking with a little les than half the hash power, and that once forking becomes profitable then one should do it all the time. However, these results involve somewhat basic strategies: either we fork or we don't fork, and there is nothing our opponent can do to prevent that. 

But what if we study more complex strategies? Is there a strategy that makes forking profitable with less hashpower than the needed for $\af$? 
It turns out this is the case, we can start forking with a little less hashpower, as long as we are willing to give up the though fights. 

The families of strategies that we study in this section involve two parameters. First, one parameter $\ell$ that mandates the maximum length of the fight we are willing to put up 
after forking, and one parameter $k$ that regulate how far back do I put my block when confronted with a chain of blocks I don't own. We denote these strategies by 
$\gup_\ell^k$. Both parameters can be seen as a refinement of the way $\af$ works, and indeed $\af$ would be equivalent to the strategy $\gup_\infty^\infty$ where 
player $1$ is willing to keep fights of any length and to fork at the beginning of any chain of blocks she does not own.  

\begin{example}
Let us consider how $\gup_4^2$ would be different from always fork. First there is the issue of where to fork. Since $k = 2$, both strategies behave the same 
for chains of cero, one or two blocks owned by player $0$. The difference is when facing a chain of 3 unowned new blocks in the blockchain, as in Figure \ref{}. 
In this case, $\af$ would try to fork from her own block, but $\gup_4^2$ would fork instead on the dotted line (i.e., the $k$-th unowned block).
Next is the give-up time. Normally, $\af$ is willing to fight forks forever, and for example, the move for state $q'$ in Figure \ref{} would still be to mine upon 
her own block $1$. On the other hand $\gup_4^2$ has already seen $4$ blocks from the start of the fork, so with this strategy player $1$ instead gives up and 
tries to mine upon the last block of player $0$, reinitiating her set of strategies as if this last block where the genesis block. 
\end{example}

To compute the closed form for $\gup_\ell^k$ we need a more involved combinatorial argument that requires counting dyck words were the difference between $0$'s and $1$'s can now be any constant, and with a bound on the maximum amount of $0$s. Let $\bgup^k_\ell$ denote the pair $(\df_0,\gup_\ell^k)$. Together with the recurrence argument of the proof of Theorem \ref{thm:always_fork}, we obtain: 

\begin{theorem}
For each pair of integers $\ell$ and $k$, the utility $u_1(\bgup^k_\ell \mid \epsilon)$ is computable, and has a closed expression of the form $u_1(\bgup^k_\ell \mid \epsilon) = \frac{\varphi}{1-\Gamma}$, where $\varphi$ and $\Gamma$ depend on $\alpha$, $\beta$, $h$, $\ell$ and $k$. 
\end{theorem}

This result gives us the possibility to mimic the plot we had for $\af$, with the same values of $\alpha$ and $\beta$s, but now for different values of $\ell$ and $k$. The most interesting results are when 
fixing $k$ and varying $\ell$, as they give us what we are looking for. As we see in Figure \ref{}, for $h = X$ the strategy $\gup_X^Y$ (color) gives higher utility 
than both the default strategy (color) and $\af$ (color). What is even more interesting, with smaller?bigger? numbers for $\ell$ we can arrive at strategies for which forking is profitable under even less hash power, albeit these strategies tend to be worse for bigger values of $h$. 

When we fix $\ell$ and vary $k$ we..... 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BELOW IS THE PREVIOUS VERSION THAT I STILL DID NOT MAKE A PASS ON %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\paragraph{Forking in the genesis block.}
The first forking strategy we models the case when 0 has won the ultimate block. The question 1 wants to answer in this situation is whether she should continue mining on the blockchain, playing according to $\df$, or should she fork, mining instead upon her previously won block? Since by the proof of Theorem \ref{thm-conts_equlibria} we know that optimal strategies for 1 depend only on the blocks following her last block in each state, we can focus on this question when the game is just starting, that is, when 1 wants to decide whether to fork in the genesis block. We will denote by $\fg_1$ the strategy in which player 1 is trying to win the first block following genesis, and once this branch becomes the new blockchain, continues mining using the default strategy. We denote by $\fg = (\df_0,\fg_1)$ the strategy where 1 tries to win the first block in the final blockchain, and 0 mines using the default strategy. A depiction of this strategy is given in Figure \ref{fig-fork_genesis}. When player 1 uses this strategy, we can compute her utility as follows.

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 1.0,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (0,0) (R) {$\varepsilon$};
	\node [state] at (1.5,0.75) (1) {$1$};
	\node [state] at (1.5,-0.75) (0) {$0$};
	\node [state] at (3,-0.75) (00) {$00$};
	
	\node [state] at (3,0.75) (11) {$11$};		
	\node [state] at (4.6,0.75) (111) {$111$};
	\node [state] at (6.4,0.75) (1110) {$1110$};
			

	
	% Graph edges
	\path[->]
	(R) edge (1)
	(R) edge (0)
	(0) edge (00)
	;  	
	
	\path[->,dashed]
	(1) edge (11)
	(11) edge (111)
	(111) edge (1110)
	;


\end{tikzpicture} 
\end{center}
\caption{Following the block 00 player 1 tries to win a fork starting at $\varepsilon$. Dashed edges show the case in which she succeeds, after which both players mine on top of the new blockchain.}
\label{fig-fork_genesis}
\end{figure}

\begin{myprop}
%	\label{prop-utilityofgenesisinfinite}
	\label{prop:utility_gen_fork}
%	Suppose player 1 plays with the genesis fork strategy and player 0 follows the default strategy. 
Let $h$ be the hash power of player 1. Then
	\begin{eqnarray*}
		u_1^\infty(\fg\mid\varepsilon) =K_1\cdot \Cat(\beta^2h(1-h))+K_2\cdot \Cat(\alpha\beta^2h(1-h))
	\end{eqnarray*}
where $\Cat:x\mapsto \frac{1-\sqrt{1-4x}}{2x}$ is the generating function of Catalan numbers \cite{ADD_CITATION}, and 
	\begin{eqnarray*}
		K_1 =\frac{\alpha\beta h}{(1-\alpha)(1-\beta)},\quad	K_2 =\frac{\alpha^2\beta h(\alpha\beta+h\beta-h\alpha\beta-1)}{(1-\alpha)(1-\beta)(1-\alpha\beta)}.	
	\end{eqnarray*}
\end{myprop}

The intuition behind the appearance of Catalan numbers in this result is that for each fixed $n$, the number of states where two chains of length $n$ are being tied for blockchain equals the $n$th Catalan number (more precisely, they correspond to Dyck paths of length $n$). Since such states dictate when 1 will win the desired fork, and no reward can be won by 1 before this occurs (note that the two branches in this game have only $\varepsilon$ in common), they tell us what the utility of player 1 will be. Notice that since $\beta \leq 1$, and the maximum of the function $h\mapsto h(1-h)$ is $\frac{1}{4}$, for $h\in [0,1]$, the value $Cat(\beta^2h(1-h))$ is always well defined. For the sake of completion, in Appendix \ref{appendix-gen_fork}, we also show what is the utility of $\fg$ for player 1 at each step $n$ of the mining game.

\paragraph{Forking $m$ blocks from the end of the blockchain.} The second forking strategy we consider works similarly as $\fg$, but now player 1 considers forking after losing $m$ consecutive blocks, for some fixed $m$. In this case, 1 will fork $m$ blocks from the end of the current blockchain, all of these $m$ block being owned by 0. This situation is depicted in Figure \ref{fig-fork_m}. Once player 1 wins the fork, and this branch becomes the new blockchain, she will continue mining according to $\df_1$. We denote this strategy of player 1 by $\forkm{m}_1$, and denote with $\forkm{m}=(\df_0,\forkm{m}_1)$ the strategy where 1 uses $\forkm{m}_1$, and 0 plays according to the default strategy.

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',auto,thick, scale = 1.0,state/.style={circle,inner sep=2pt}]

    % The graph
	\node [state] at (0,0) (R) {$\varepsilon$};
	\node [state] at (1.5,0) (0) {$0$};
	\node [state] at (3,0) (01) {$01$};
	\node [state] at (4.6,0) (010) {$010$};
	\node [state] at (6.4,0) (0100) {$0100$};
	
	\node [state] at (4.6,0.75) (011) {$011$};					
	
	% Graph edges
	\path[->]
	(R) edge (0)
	(0) edge (01)
	(01) edge (010)
	(010) edge (0100)	
	;  	
	
	\path[->,dashed]
	(01) edge (011)
	;


\end{tikzpicture} 
\end{center}
\caption{If playing $\forkm{2}_1$, player 1 will try to fork after losing two consecutive blocks at position 0100. Should she manage to make the dashed branch into a blockchain, both her and player 1 continue to mine on this branch using $\df$.}
\label{fig-fork_m}
\end{figure}

When computing the utility of $\forkm{m}$ starting in some state $q_0$, we take into account that player 1 is only interested whether her utility will increase in the case she decides to fork. That is, since the blocks before her fork will always have the same constant contribution to her reward in either branch, we will not count them in the utility. With this in mind, the utility of $\forkm{m}$ starting in the state $q_0$ in which 0 has the ultimate $m$ blocks in the (up to now unique) blockchain is given as follows.

\begin{myprop}\label{prop-fork_m}
	Let $h$ be the hash power of player 1. Then
	\begin{eqnarray*}
		u_1(\mfork|q_0) =\bigg(\frac{\beta h}{\alpha}\bigg)^m K_{1}\Cat(\beta^2h(1-h))^{m+1}+(\beta h)^{m}K_{2}\Cat(\alpha\beta^2h(1-h))^{m+1}
	\end{eqnarray*}
	where $\Cat$ is as in Proposition \ref{prop:utility_gen_fork}, and 

\end{myprop}

As before, Catalan numbers help us account for all states where 1 wins the fork. Of course, now player 1 is starting with a handicap of $m$ steps, so the winning states for 1 will be characterized by staircase walks that stay inside the trapezoid $\{(0,0),(m,0),(m+n,n),(0,n)\}$, for some number $n$. The proof of this proposition is given in Appendix \ref{appendix-fork_m}.

\paragraph{When to fork?} Having the utilities of the two forking strategies, we can now compare them to that of the default strategy in order to answer whether 1 should fork or not. Analysing the curves defined by the game parameters ($\alpha,\beta$ and $h$ for $\fg$, and additionally $m$ for $\forkm{m}$), it can be seen that when $h>50\%$, and $\alpha>\beta$, then it is always convenient for 1 to fork, based on the utility resulting from either $\fg$ or (irrespective of $m$) $\forkm{m}$. This confirms the intuition that the player with the majority of hash power can sway the game in her favour, but we can also see that there are several cases when it is convenient to fork even with much lower hash power. This happens in cases when $\alpha$ is significantly bigger than $\beta$, which can be explained by the fact that ???

\end{comment}
